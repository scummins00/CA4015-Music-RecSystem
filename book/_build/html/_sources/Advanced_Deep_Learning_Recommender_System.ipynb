{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b2495aa",
   "metadata": {},
   "source": [
    "# Advanced Deep Learning Recommender System\n",
    "In the following notebook, we will be creating another Deep-Learning Recommender using Tensorflow V2. For this iteration, we will try to incorporate text and timestamp data available to us. As already stated multiple times, the tags in this data are user-generated. Therefore, they are messy, inconsistent, and may not be entirely accurate and or useful. \n",
    "\n",
    "The TFRS package is incredibly robust, and offers plenty of direction for expansion of recommender systems. The library can tokenize text and timestamps into features. It processes text into a 'bag-of-words' representation, which it can then use to find similarities. It will be interesting to see if this approach alters recommendations to be affected more by the genre or tags associated with artists.\n",
    "\n",
    "Similarly, it will be interesting to see how the inclusion of temporal data changes recommendations. In our data, a timestamp is associated with a *user*, *artist*, and *tag*. It indicates the exact time that particular user gave that artist that tag. It is entirely possible that amongst the tag information users who do not like particular artists have left negative tags. I wonder if an association will be made between few listens (*low weight*) and particular tag tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b024fc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "\n",
    "from typing import Dict, Text\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow_recommenders as tfrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9575cc",
   "metadata": {},
   "source": [
    "### Accounting for Tag Information\n",
    "In the following cell, we will prove that in our data, users can tag the same artist multiple times. Preferably, we would like only 1 tag for any user-artist association. We will order a user-artist tags dataset by their creation time and use the most recent tag and timestamp for each user-artist combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89347fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contains Duplicate user-artist combinations.\n"
     ]
    }
   ],
   "source": [
    "#Let's read in genres and tags\n",
    "tags = pd.read_csv('../data/tags.dat', sep='\\t', encoding='latin-1')\n",
    "user_tagging = pd.read_csv('../data/user_taggedartists.dat', sep='\\t', encoding='latin-1')\n",
    "user_tagging_time = pd.read_csv('../data/user_taggedartists-timestamps.dat', sep='\\t', encoding='latin-1')\n",
    "\n",
    "#Check if duplicates are present\n",
    "if True in user_tagging_time[['userID', 'artistID']].duplicated():\n",
    "    print(\"Contains Duplicate user-artist combinations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e24324d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 3 random samples for tag data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>artistID</th>\n",
       "      <th>tagID</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>tagValue</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31475</th>\n",
       "      <td>1191</td>\n",
       "      <td>3921</td>\n",
       "      <td>73</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2008</td>\n",
       "      <td>rock</td>\n",
       "      <td>1204326000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11861</th>\n",
       "      <td>1191</td>\n",
       "      <td>18391</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2007</td>\n",
       "      <td>dance</td>\n",
       "      <td>1183240800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59146</th>\n",
       "      <td>1534</td>\n",
       "      <td>1201</td>\n",
       "      <td>127</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>seen live</td>\n",
       "      <td>1228086000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       userID  artistID  tagID  day  month  year   tagValue      timestamp\n",
       "31475    1191      3921     73    1      3  2008       rock  1204326000000\n",
       "11861    1191     18391     39    1      7  2007      dance  1183240800000\n",
       "59146    1534      1201    127    1     12  2008  seen live  1228086000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_tag_a = user_tagging.merge(tags[['tagID', 'tagValue']], on='tagID')\n",
    "u_tag_a = u_tag_a.merge(user_tagging_time, on=['userID', 'artistID', 'tagID'])\n",
    "print(\"Displaying 3 random samples for tag data:\")\n",
    "u_tag_a.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42516c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group by user-artist combo, sort by timestamp and extract that tagValue\n",
    "u_tag_a = u_tag_a.sort_values(by='timestamp', ascending=False).groupby(['userID', 'artistID']).first().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d05ae16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does not contain Duplicate user-artist combinations.\n"
     ]
    }
   ],
   "source": [
    "#Let's check if this dataset contains duplicate user-artist combinations\n",
    "if True in np.unique(u_tag_a[['userID', 'artistID']].duplicated().values):\n",
    "    print(\"Contains Duplicate user-artist combinations.\")\n",
    "else:\n",
    "    print(\"Does not contain Duplicate user-artist combinations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1ba109",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "Our preprocessing steps are as before for the most part. For a final step, we will merge our tag information dataset with our ratings matrix. To start, we normalise our weight column as previous.\n",
    "\n",
    "There will be many cases where a user listens to a particular artist, but never provides that artist with a tag. In those cases, we will let the tag value be `no tag`, and for the corresponding timestamp value, we will use a value corresponding to today. We obviously want our model to find associations between users and common tags. However, our model can also build associations in situations where a user has decided to not provide a tag.\n",
    "\n",
    "The timestamps provided in the dataset do not correspond to the correct year and must have the final 3 digits removed. For this, we can just divide them all by 1,000. Using [this website](https://timestamp.online/), I entered some of the corrected timestamps to ensure they do indeed correspond to the appropriate year. All entries I checked returned values around 2008 to 2011, which makes sense for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3ace463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "#Correct timestamp data in u_tag_a\n",
    "u_tag_a['timestamp'] = u_tag_a['timestamp'].apply(lambda x: math.floor(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3830e37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's define our amount of users\n",
    "rating_matrix = pd.read_csv('../data/user_artists.dat', sep='\\t', encoding='latin-1')\n",
    "num_users = len(rating_matrix.userID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75b82646",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanc\\AppData\\Local\\Temp/ipykernel_15628/1239368141.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  user_ratings['weight'] = tf.keras.utils.normalize(ratings, axis=-1, order=2)[0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>92834.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.091235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.109804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.030397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.062543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.109109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             weight\n",
       "count  92834.000000\n",
       "mean       0.091235\n",
       "std        0.109804\n",
       "min        0.000008\n",
       "25%        0.030397\n",
       "50%        0.062543\n",
       "75%        0.109109\n",
       "max        1.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's normalise our weight column per user\n",
    "new_rating_matrix = pd.DataFrame(columns=['userID', 'artistID', 'weight'])\n",
    "for user_id in rating_matrix.userID.unique():\n",
    "    user_ratings = rating_matrix[rating_matrix.userID == user_id]\n",
    "    ratings = np.array(user_ratings['weight'])\n",
    "    user_ratings['weight'] = tf.keras.utils.normalize(ratings, axis=-1, order=2)[0]\n",
    "    new_rating_matrix = new_rating_matrix.append(user_ratings)\n",
    "rating_matrix = new_rating_matrix\n",
    "rating_matrix.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55f9ac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's use left merge to merge our tag data and rating matrix\n",
    "rating_matrix = rating_matrix.merge(u_tag_a[['userID', 'artistID', 'tagValue', 'timestamp']],\n",
    "                                    on=['userID', 'artistID'], how='left')\n",
    "\n",
    "#Get today's timestamp\n",
    "now = math.floor(time.time())\n",
    "\n",
    "#Fill missing values as stated\n",
    "rating_matrix.tagValue = rating_matrix['tagValue'].fillna('no tag')\n",
    "rating_matrix.timestamp = rating_matrix['timestamp'].fillna(now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcbcdd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying Sample of new Rating Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>artistID</th>\n",
       "      <th>weight</th>\n",
       "      <th>tagValue</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57166</th>\n",
       "      <td>1279</td>\n",
       "      <td>1736</td>\n",
       "      <td>0.097961</td>\n",
       "      <td>instrumental</td>\n",
       "      <td>1.298934e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28582</th>\n",
       "      <td>625</td>\n",
       "      <td>416</td>\n",
       "      <td>0.091545</td>\n",
       "      <td>alternative rock</td>\n",
       "      <td>1.293836e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56194</th>\n",
       "      <td>1254</td>\n",
       "      <td>13498</td>\n",
       "      <td>0.372580</td>\n",
       "      <td>chillout</td>\n",
       "      <td>1.257030e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8003</th>\n",
       "      <td>171</td>\n",
       "      <td>2521</td>\n",
       "      <td>0.047412</td>\n",
       "      <td>female vocalists</td>\n",
       "      <td>1.293836e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41726</th>\n",
       "      <td>920</td>\n",
       "      <td>1185</td>\n",
       "      <td>0.172471</td>\n",
       "      <td>post-hardcore</td>\n",
       "      <td>1.291158e+12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      userID artistID    weight          tagValue     timestamp\n",
       "57166   1279     1736  0.097961      instrumental  1.298934e+12\n",
       "28582    625      416  0.091545  alternative rock  1.293836e+12\n",
       "56194   1254    13498  0.372580          chillout  1.257030e+12\n",
       "8003     171     2521  0.047412  female vocalists  1.293836e+12\n",
       "41726    920     1185  0.172471     post-hardcore  1.291158e+12"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Displaying Sample of new Rating Matrix\")\n",
    "rating_matrix[rating_matrix.tagValue != 'no tag'].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22451696",
   "metadata": {},
   "source": [
    "The small sample above gives an indication for some of the values we can expect to find for tags. There is a large amount of distinct values in our tag data. It will be interesting to see how the recommender system interprets these.\n",
    "\n",
    "The below pre-processing steps are as before in our other notebooks. We are correcting the scale of user and artist ID's, then ensuring their maximum values are appropriate before replacing the columns in our rating matrix.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15926ad",
   "metadata": {},
   "source": [
    "### Artist Preprocessing\n",
    "To make use of the tags generally associated with artists, we will calculate their most popular tag in our data. We will use this information later on when developing our candidate model. The function in the cell below performs as previous. Essentially, it finds the most popular tag for each artist and attaches it to their profile.\n",
    "\n",
    "We will add this extra information to our ratings matrix, as well as the artists name. Using the artist name as an identifier will make more sense to us than an ID number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a886bd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's match artists to genres\n",
    "artists = pd.read_csv('../data/artists.dat', sep='\\t', encoding='latin-1')\n",
    "artists_tagged = user_tagging.merge(tags[['tagID', 'tagValue']], on='tagID')\n",
    "artists_tagged = (artists_tagged.groupby('artistID')['tagValue'].apply(lambda grp: list(grp))).reset_index()\n",
    "\n",
    "#This function performs as previous.\n",
    "for index, row in artists_tagged.iterrows():\n",
    "    d = {}\n",
    "    new_tags = []\n",
    "    for val in row.tagValue:\n",
    "        if val not in d:\n",
    "            d[val] = 1\n",
    "        else:\n",
    "            d[val] += 1\n",
    "    for key, value in d.items():\n",
    "        if d[key] >=3:\n",
    "            new_tags.append([key, value])\n",
    "    new_tags.sort(key=lambda x:x[1], reverse=True)\n",
    "    if new_tags:\n",
    "        artists_tagged.at[index, \"tagValue\"] = [tag[0] for tag in new_tags]\n",
    "        artists_tagged.at[index, 'genre'] = artists_tagged.at[index, 'tagValue'][0]\n",
    "        \n",
    "#Let's add these tags to our artists\n",
    "artists.rename(columns={'id':'artistID'}, inplace=True)\n",
    "artists = artists.join(artists_tagged, on='artistID', how='left', rsuffix='right')\n",
    "artists.tagValue = artists.tagValue.fillna('No Tags')\n",
    "artists.genre = artists.genre.fillna('No Tags')\n",
    "artists.rename(columns={'tagValue': 'genres'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "912db48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We add the extra info to our ratings matrix\n",
    "rating_matrix = rating_matrix.merge(artists[['artistID', 'name', 'genre']], on='artistID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71cc426e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract userID column\n",
    "userids = np.asarray(rating_matrix.userID)\n",
    "\n",
    "#Remap the column\n",
    "u_mapper, u_ind = np.unique(userids, return_inverse=True)\n",
    "\n",
    "#Let's define our amount of artists\n",
    "artists = pd.read_csv('../data/artists.dat', sep='\\t', encoding='latin-1')\n",
    "artists.rename(columns={'id':'artistID'}, inplace=True)\n",
    "num_artists = len(artists.artistID.unique())\n",
    "\n",
    "#Extract artistID column\n",
    "artistids = np.asarray(rating_matrix.artistID)\n",
    "\n",
    "#Remap the column\n",
    "a_mapper, a_ind = np.unique(artistids, return_inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b69e0996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's replace old columns with new ind ones\n",
    "rating_matrix.userID = u_ind\n",
    "rating_matrix.artistID = a_ind\n",
    "\n",
    "#Let's ensure the max value is approriate\n",
    "assert(rating_matrix.userID.unique().max() == 1891)\n",
    "assert(rating_matrix.artistID.unique().max() == 17631)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f62a0776",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We convert the ID's to string so we can use the StringLookup function later\n",
    "rating_matrix.userID = rating_matrix.userID.apply(str)\n",
    "rating_matrix.artistID = rating_matrix.artistID.apply(str)\n",
    "\n",
    "rating_matrix.timestamp = rating_matrix.timestamp.apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b4a6d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>artistID</th>\n",
       "      <th>weight</th>\n",
       "      <th>tagValue</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>name</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59870</th>\n",
       "      <td>815</td>\n",
       "      <td>2588</td>\n",
       "      <td>0.077078</td>\n",
       "      <td>no tag</td>\n",
       "      <td>1638553930</td>\n",
       "      <td>City and Colour</td>\n",
       "      <td>No Tags</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80338</th>\n",
       "      <td>1358</td>\n",
       "      <td>8483</td>\n",
       "      <td>0.044851</td>\n",
       "      <td>no tag</td>\n",
       "      <td>1638553930</td>\n",
       "      <td>Karunesh</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81141</th>\n",
       "      <td>623</td>\n",
       "      <td>8933</td>\n",
       "      <td>0.005332</td>\n",
       "      <td>brazilian</td>\n",
       "      <td>1291158000000</td>\n",
       "      <td>Marcos e Belutti</td>\n",
       "      <td>No Tags</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41710</th>\n",
       "      <td>176</td>\n",
       "      <td>1034</td>\n",
       "      <td>0.015823</td>\n",
       "      <td>no tag</td>\n",
       "      <td>1638553930</td>\n",
       "      <td>Boys Like Girls</td>\n",
       "      <td>No Tags</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10257</th>\n",
       "      <td>1646</td>\n",
       "      <td>223</td>\n",
       "      <td>0.141574</td>\n",
       "      <td>indie rock</td>\n",
       "      <td>1196463600000</td>\n",
       "      <td>The Killers</td>\n",
       "      <td>female vocalists</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      userID artistID    weight    tagValue      timestamp              name  \\\n",
       "59870    815     2588  0.077078      no tag     1638553930   City and Colour   \n",
       "80338   1358     8483  0.044851      no tag     1638553930          Karunesh   \n",
       "81141    623     8933  0.005332   brazilian  1291158000000  Marcos e Belutti   \n",
       "41710    176     1034  0.015823      no tag     1638553930   Boys Like Girls   \n",
       "10257   1646      223  0.141574  indie rock  1196463600000       The Killers   \n",
       "\n",
       "                  genre  \n",
       "59870           No Tags  \n",
       "80338           country  \n",
       "81141           No Tags  \n",
       "41710           No Tags  \n",
       "10257  female vocalists  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_matrix.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f52b9e1",
   "metadata": {},
   "source": [
    "## Model Development\n",
    "We will perform the steps to developing a model as previous. However, we will now utilise our tags and timestamps. We do this by instantiating our interactions dictionary and including the extra features.\n",
    "\n",
    "### Instantiate Interaction Dictionary\n",
    "Our interactions dictionary is just our rating matrix. It contains the following features `userID`, `artistID`, `name`, `weight`, `tag`, `genre`, and `timestamp`. We create a mapping for our dictionary below. We also create seperate individual mappings for artist ID's, tags, and genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e693e1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's build our interactions dictionary as previous\n",
    "interactions_dict = {name: np.array(value) for name, value in rating_matrix.items()}\n",
    "interactions = tf.data.Dataset.from_tensor_slices(interactions_dict)\n",
    "\n",
    "items_dict = rating_matrix[['artistID']].drop_duplicates()\n",
    "items_dict = {name: np.array(value) for name, value in items_dict.items()}\n",
    "items = tf.data.Dataset.from_tensor_slices(items_dict)\n",
    "\n",
    "names_dict = rating_matrix[['name']].drop_duplicates()\n",
    "names_dict = {name: np.array(value) for name, value in names_dict.items()}\n",
    "names = tf.data.Dataset.from_tensor_slices(names_dict)\n",
    "\n",
    "tags_dict = rating_matrix[['tagValue']].drop_duplicates()\n",
    "tags_dict = {name: np.array(value) for name, value in tags_dict.items()}\n",
    "tags = tf.data.Dataset.from_tensor_slices(tags_dict)\n",
    "\n",
    "genre_dict = rating_matrix[['genre']].drop_duplicates()\n",
    "genre_dict = {name:np.array(value) for name, value in genre_dict.items()}\n",
    "genres = tf.data.Dataset.from_tensor_slices(genre_dict)\n",
    "\n",
    "interactions = interactions.map(lambda x: {\n",
    "                                            'userID' : x['userID'], \n",
    "                                            'artistID' : x['artistID'], \n",
    "                                            'name' : x['name'],\n",
    "                                            'weight' : float(x['weight']),\n",
    "                                            'tag' : x['tagValue'],\n",
    "                                            'genre': x['genre'],\n",
    "                                            'timestamp': x[\"timestamp\"],})\n",
    "\n",
    "#artists = names.map(lambda x: x['name'])\n",
    "items = items.map(lambda x: x['artistID'])\n",
    "tags = tags.map(lambda x: x['tagValue'])\n",
    "genres = genres.map(lambda x: x['genre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520cdadc",
   "metadata": {},
   "source": [
    "### Timestamp Normalisation\n",
    "As timestamps are represented as large integers, they are not healthy to use as direct input into our model. We firstly normalise our timestamps by calculaitng our minimum and maximum timestamp, then creating buckets at equal intervals between these two times. We instantiate 1000 buckets which are used to host our timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72b780cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's create bins for our timestamps\n",
    "max_timestamp = interactions.map(lambda x: x[\"timestamp\"]).reduce(\n",
    "    tf.cast(0, tf.int64), tf.maximum).numpy().max()\n",
    "\n",
    "min_timestamp = interactions.map(lambda x: x[\"timestamp\"]).reduce(\n",
    "    np.int64(1e9), tf.minimum).numpy().min()\n",
    "\n",
    "timestamp_buckets = np.linspace( min_timestamp, max_timestamp, num=1000,)\n",
    "\n",
    "timestamps = interactions.map(lambda x: x[\"timestamp\"]).batch(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e82e1c",
   "metadata": {},
   "source": [
    "### Lookup Tables & Training, Test Data Split\n",
    "In the following cell, we define various lookup tables which we may use later on. We also shuffle our data and create testing and training batches which will be fed into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4bb7993a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### get unique item and user id's as a lookup table\n",
    "unique_artist_ids = (np.unique(a_ind)).astype(str)\n",
    "unique_user_ids = (np.unique(u_ind)).astype(str)\n",
    "unique_genre_ids = np.unique(rating_matrix.genre)\n",
    "unique_user_tags = np.unique(rating_matrix.tagValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d4a94b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly shuffle data and split between train and test.\n",
    "tf.random.set_seed(42)\n",
    "shuffled = interactions.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(62_000)\n",
    "test = shuffled.skip(62_000).take(30_000)\n",
    "\n",
    "cached_train = train.shuffle(62_000).batch(5_000)\n",
    "cached_test = test.batch(2_500).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b705f69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our test set is: 62000\n",
      "our train set is: 30000\n"
     ]
    }
   ],
   "source": [
    "print(f'our test set is: {len(train)}')\n",
    "print(f'our train set is: {len(test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0043bd7f",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "The below cells host a more complex version of the model we saw previously. In our previous model, we simply instantiated our user and item embeddings as we would in a regular collaborative filtering model. In this instance, we further develop our user and item models.\n",
    "\n",
    "---\n",
    "\n",
    "### User Model\n",
    "In the below cell, we develop our user model. We incorporate the user ID, as well as the timestamp data. As the timestamp data signifies when a user provided a tag to an artist, it is more suitably found in the user model.\n",
    "\n",
    "In our user model, we have included a parameter, `_use_timestamps`. When set to true, the model incorporates time stamp information. This will allow us to compare the results of the model with, or without the use of timestamps.\n",
    "\n",
    "In our dataset, it's hard to interpret the utility of timestamps. This is because timestamp information is related to when the user-provided tags were actually applied to the artist, rather than when the user posted the tag. Also, there is an argument that including timestamps of today's date may have negative effects on the model. This model is trained on information from 2009 to 2011, essentially making it a model of that period. Timestamps from today allow the model to 'see into the future', which is obviously not a realistic trait of ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3511dd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "### user model\n",
    "\n",
    "class UserModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, use_timestamps):\n",
    "        super().__init__()\n",
    "        max_tokes=25_000\n",
    "\n",
    "        self._use_timestamps = use_timestamps\n",
    "\n",
    "        ## embed user id from unique_user_ids\n",
    "        self.user_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "                vocabulary=unique_user_ids),\n",
    "            tf.keras.layers.Embedding(len(unique_user_ids) + 1, 32),\n",
    "        ])\n",
    "        \n",
    "        ## embed timestamp\n",
    "        if use_timestamps:\n",
    "            self.timestamp_embedding = tf.keras.Sequential([\n",
    "              tf.keras.layers.experimental.preprocessing.Discretization(timestamp_buckets.tolist()),\n",
    "              tf.keras.layers.Embedding(len(timestamp_buckets) + 1, 32),\n",
    "            ])\n",
    "            self.normalized_timestamp = tf.keras.layers.experimental.preprocessing.Normalization(axis=None)\n",
    "            self.normalized_timestamp.adapt(timestamps)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if not self._use_timestamps:\n",
    "              return self.user_embedding(inputs[\"userID\"])\n",
    "\n",
    "        ## all features here\n",
    "        return tf.concat([\n",
    "            self.user_embedding(inputs[\"userID\"]),\n",
    "            self.timestamp_embedding(inputs[\"timestamp\"]),\n",
    "            tf.reshape(self.normalized_timestamp(inputs[\"timestamp\"]), (-1, 1)),\n",
    "        ], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10663b35",
   "metadata": {},
   "source": [
    "### Item Model\n",
    "Our item model incorporates our artist ID as before. However, it also makes use of the genre associated with the artist. \n",
    "\n",
    "To make use of genre strings, we must first instantiate our `genre_vectorizer`. This will allow us to convert our genre string into a numerical representation. The `genre_vectorizer` is then used by our `genre_text_embedding` processing step to create an embedding of this word vector. Word embeddings allow us to measure similarity between text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e1081375",
   "metadata": {},
   "outputs": [],
   "source": [
    "### candidate model\n",
    "\n",
    "class ItemModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        max_tokens = 10_000\n",
    "        \n",
    "        ## embed artist id from unique_artist_ids\n",
    "        self.artist_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "                vocabulary=unique_artist_ids),\n",
    "            tf.keras.layers.Embedding(len(unique_artist_ids) + 1, 32),])\n",
    "        \n",
    "        ## processing text features: item genre vectorizer\n",
    "        self.artist_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "            max_tokens=max_tokens)\n",
    "\n",
    "        ## we apply genre vectorizer to genres\n",
    "        self.artist_text_embedding = tf.keras.Sequential([\n",
    "                              self.artist_vectorizer,\n",
    "                              tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),\n",
    "                              tf.keras.layers.GlobalAveragePooling1D(),])\n",
    "\n",
    "        self.artist_vectorizer.adapt(genres)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.concat([\n",
    "            self.artist_embedding(inputs[\"artistID\"]),\n",
    "            self.artist_text_embedding(inputs[\"genre\"]),], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2e2a5e",
   "metadata": {},
   "source": [
    "### Combining Models\n",
    "The following cell is our parent model which we use to combine the output of both our User and Item models. We feed the outputs of each model into two dense embedding layers both of the same shape (*32*). \n",
    "\n",
    "We then define our task (in this case FactorizedTopK), then compute the loss as we did previously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9789ee96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicModel(tfrs.models.Model):\n",
    "    def __init__(self, use_timestamps):\n",
    "        super().__init__()\n",
    "\n",
    "        ## query model is user model\n",
    "        self.query_model = tf.keras.Sequential([\n",
    "                          UserModel(use_timestamps),\n",
    "                          tf.keras.layers.Dense(32)])\n",
    "        \n",
    "        ## candidate model is the item model\n",
    "        self.candidate_model = tf.keras.Sequential([\n",
    "                              ItemModel(),\n",
    "                              tf.keras.layers.Dense(32)])\n",
    "        \n",
    "        ## retrieval task, choose metrics\n",
    "        self.task = tfrs.tasks.Retrieval(\n",
    "                    metrics=tfrs.metrics.FactorizedTopK(\n",
    "                        candidates=items.batch(128).map(self.candidate_model),),)\n",
    "\n",
    "    def compute_loss(self, features, training=False):\n",
    "        # We only pass the user id and timestamp features into the query model. This\n",
    "        # is to ensure that the training inputs would have the same keys as the\n",
    "        # query inputs. Otherwise the discrepancy in input structure would cause an\n",
    "        # error when loading the query model after saving it.\n",
    "        \n",
    "        query_embeddings = self.query_model({ \"userID\": features[\"userID\"],\n",
    "                                               \"timestamp\": features[\"timestamp\"],\n",
    "                                                \"tag\": features[\"tag\"],\n",
    "                                            })\n",
    "        \n",
    "        item_embeddings = self.candidate_model(features[\"genre\"])\n",
    "\n",
    "        return self.task(query_embeddings, item_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071b5984",
   "metadata": {},
   "source": [
    "### Model Fitting and Evaluation\n",
    "In the following cells, we will perform two different fitting and evaluation scenarios: $(a)$ `_use_timestamps = True`, and $(b)$ `_use_timestamps = False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b060342b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'userID': <tf.Tensor 'IteratorGetNext:5' shape=(None,) dtype=string>, 'timestamp': <tf.Tensor 'IteratorGetNext:4' shape=(None,) dtype=int64>, 'tag': <tf.Tensor 'IteratorGetNext:3' shape=(None,) dtype=string>}\n",
      "Consider rewriting this model with the Functional API.\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'userID': <tf.Tensor 'IteratorGetNext:5' shape=(None,) dtype=string>, 'timestamp': <tf.Tensor 'IteratorGetNext:4' shape=(None,) dtype=int64>, 'tag': <tf.Tensor 'IteratorGetNext:3' shape=(None,) dtype=string>}\n",
      "Consider rewriting this model with the Functional API.\n",
      "13/13 [==============================] - 58s 4s/step - factorized_top_k/top_1_categorical_accuracy: 0.3392 - factorized_top_k/top_5_categorical_accuracy: 0.3576 - factorized_top_k/top_10_categorical_accuracy: 0.3658 - factorized_top_k/top_50_categorical_accuracy: 0.3871 - factorized_top_k/top_100_categorical_accuracy: 0.3995 - loss: 38700.8058 - regularization_loss: 0.0000e+00 - total_loss: 38700.8058\n",
      "Epoch 2/3\n",
      "13/13 [==============================] - 60s 4s/step - factorized_top_k/top_1_categorical_accuracy: 0.5086 - factorized_top_k/top_5_categorical_accuracy: 0.5241 - factorized_top_k/top_10_categorical_accuracy: 0.5296 - factorized_top_k/top_50_categorical_accuracy: 0.5460 - factorized_top_k/top_100_categorical_accuracy: 0.5541 - loss: 38416.6585 - regularization_loss: 0.0000e+00 - total_loss: 38416.6585\n",
      "Epoch 3/3\n",
      "13/13 [==============================] - 62s 4s/step - factorized_top_k/top_1_categorical_accuracy: 0.5997 - factorized_top_k/top_5_categorical_accuracy: 0.6118 - factorized_top_k/top_10_categorical_accuracy: 0.6175 - factorized_top_k/top_50_categorical_accuracy: 0.6304 - factorized_top_k/top_100_categorical_accuracy: 0.6372 - loss: 38141.4827 - regularization_loss: 0.0000e+00 - total_loss: 38141.4827\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'userID': <tf.Tensor 'IteratorGetNext:5' shape=(None,) dtype=string>, 'timestamp': <tf.Tensor 'IteratorGetNext:4' shape=(None,) dtype=int64>, 'tag': <tf.Tensor 'IteratorGetNext:3' shape=(None,) dtype=string>}\n",
      "Consider rewriting this model with the Functional API.\n",
      "12/12 [==============================] - 28s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.5477 - factorized_top_k/top_5_categorical_accuracy: 0.5613 - factorized_top_k/top_10_categorical_accuracy: 0.5678 - factorized_top_k/top_50_categorical_accuracy: 0.5814 - factorized_top_k/top_100_categorical_accuracy: 0.5872 - loss: 19454.4151 - regularization_loss: 0.0000e+00 - total_loss: 19454.4151\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'factorized_top_k/top_1_categorical_accuracy': 0.5477333068847656,\n",
       " 'factorized_top_k/top_5_categorical_accuracy': 0.5612666606903076,\n",
       " 'factorized_top_k/top_10_categorical_accuracy': 0.5678333044052124,\n",
       " 'factorized_top_k/top_50_categorical_accuracy': 0.5813999772071838,\n",
       " 'factorized_top_k/top_100_categorical_accuracy': 0.587233304977417,\n",
       " 'loss': 19470.15234375,\n",
       " 'regularization_loss': 0,\n",
       " 'total_loss': 19470.15234375}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MusicModel(use_timestamps=False)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "model.fit(cached_train, epochs=3)\n",
    "model.evaluate(cached_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1471c302",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'userID': <tf.Tensor 'IteratorGetNext:5' shape=(None,) dtype=string>, 'timestamp': <tf.Tensor 'IteratorGetNext:4' shape=(None,) dtype=int64>, 'tag': <tf.Tensor 'IteratorGetNext:3' shape=(None,) dtype=string>}\n",
      "Consider rewriting this model with the Functional API.\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'userID': <tf.Tensor 'IteratorGetNext:5' shape=(None,) dtype=string>, 'timestamp': <tf.Tensor 'IteratorGetNext:4' shape=(None,) dtype=int64>, 'tag': <tf.Tensor 'IteratorGetNext:3' shape=(None,) dtype=string>}\n",
      "Consider rewriting this model with the Functional API.\n",
      "13/13 [==============================] - 62s 4s/step - factorized_top_k/top_1_categorical_accuracy: 0.2838 - factorized_top_k/top_5_categorical_accuracy: 0.3035 - factorized_top_k/top_10_categorical_accuracy: 0.3124 - factorized_top_k/top_50_categorical_accuracy: 0.3349 - factorized_top_k/top_100_categorical_accuracy: 0.3480 - loss: 39029.6232 - regularization_loss: 0.0000e+00 - total_loss: 39029.6232\n",
      "Epoch 2/3\n",
      "13/13 [==============================] - 60s 4s/step - factorized_top_k/top_1_categorical_accuracy: 0.3954 - factorized_top_k/top_5_categorical_accuracy: 0.4175 - factorized_top_k/top_10_categorical_accuracy: 0.4273 - factorized_top_k/top_50_categorical_accuracy: 0.4529 - factorized_top_k/top_100_categorical_accuracy: 0.4650 - loss: 38513.8764 - regularization_loss: 0.0000e+00 - total_loss: 38513.8764\n",
      "Epoch 3/3\n",
      "13/13 [==============================] - 63s 5s/step - factorized_top_k/top_1_categorical_accuracy: 0.5219 - factorized_top_k/top_5_categorical_accuracy: 0.5371 - factorized_top_k/top_10_categorical_accuracy: 0.5441 - factorized_top_k/top_50_categorical_accuracy: 0.5613 - factorized_top_k/top_100_categorical_accuracy: 0.5698 - loss: 38298.1392 - regularization_loss: 0.0000e+00 - total_loss: 38298.1392\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'userID': <tf.Tensor 'IteratorGetNext:5' shape=(None,) dtype=string>, 'timestamp': <tf.Tensor 'IteratorGetNext:4' shape=(None,) dtype=int64>, 'tag': <tf.Tensor 'IteratorGetNext:3' shape=(None,) dtype=string>}\n",
      "Consider rewriting this model with the Functional API.\n",
      "12/12 [==============================] - 30s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.4899 - factorized_top_k/top_5_categorical_accuracy: 0.5035 - factorized_top_k/top_10_categorical_accuracy: 0.5103 - factorized_top_k/top_50_categorical_accuracy: 0.5257 - factorized_top_k/top_100_categorical_accuracy: 0.5324 - loss: 19461.5293 - regularization_loss: 0.0000e+00 - total_loss: 19461.5293\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'factorized_top_k/top_1_categorical_accuracy': 0.4898666739463806,\n",
       " 'factorized_top_k/top_5_categorical_accuracy': 0.5034999847412109,\n",
       " 'factorized_top_k/top_10_categorical_accuracy': 0.5102666616439819,\n",
       " 'factorized_top_k/top_50_categorical_accuracy': 0.5256999731063843,\n",
       " 'factorized_top_k/top_100_categorical_accuracy': 0.5324333310127258,\n",
       " 'loss': 19464.4296875,\n",
       " 'regularization_loss': 0,\n",
       " 'total_loss': 19464.4296875}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MusicModel(use_timestamps=True)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "model.fit(cached_train, epochs=3)\n",
    "model.evaluate(cached_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25440a7",
   "metadata": {},
   "source": [
    "### Output Analysis\n",
    "From the above output, it's clear that timestamps do not improve recommendations for this model. This is likely due to the fact that a substantial amount of the timestamps are of today's date which is throwing off the model. Perhaps a better data imputation would have been the median date observed in the data.\n",
    "\n",
    "Otherwise both models seem to have reasonable performance with a positive item being returned as the top candidate 50% of the time. These models will supply a basis for our next advanced deep retrieval model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
