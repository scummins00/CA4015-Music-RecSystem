{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b2495aa",
   "metadata": {},
   "source": [
    "# Advanced Deep Learning Recommender System\n",
    "In the following notebook, we will be creating another Deep-Learning Recommender using Tensorflow V2. For this iteration, we will try to incorporate text and timestamp data available to us. As already stated multiple times, the tags in this data are user-generated. Therefore, they are messy, inconsistent, and may not be entirely accurate and or useful. \n",
    "\n",
    "The TFRS package is incredibly robust, and offers plenty of direction for expansion of recommender systems. The library can tokenize text and timestamps into features. It processes text into a 'bag-of-words' representation, which it can then use to find similarities. It will be interesting to see if this approach alters recommendations to be affected more by the genre or tags associated with artists.\n",
    "\n",
    "Similarly, it will be interesting to see how the inclusion of temporal data changes recommendations. In our data, a timestamp is associated with a *user*, *artist*, and *tag*. It indicates the exact time that particular user gave that artist that tag. It is entirely possible that amongst the tag information users who do not like particular artists have left negative tags. I wonder if an association will be made between few listens (*low weight*) and particular tag tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b024fc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "\n",
    "from typing import Dict, Text\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow_recommenders as tfrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9575cc",
   "metadata": {},
   "source": [
    "### Accounting for Tag Information\n",
    "In the following cell, we will prove that in our data, users can tag the same artist multiple times. Preferably, we would like only 1 tag for any user-artist association. We will order a user-artist tags dataset by their creation time and use the most recent tag and timestamp for each user-artist combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89347fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contains Duplicate user-artist combinations.\n"
     ]
    }
   ],
   "source": [
    "#Let's read in genres and tags\n",
    "tags = pd.read_csv('../data/tags.dat', sep='\\t', encoding='latin-1')\n",
    "user_tagging = pd.read_csv('../data/user_taggedartists.dat', sep='\\t', encoding='latin-1')\n",
    "user_tagging_time = pd.read_csv('../data/user_taggedartists-timestamps.dat', sep='\\t', encoding='latin-1')\n",
    "\n",
    "#Check if duplicates are present\n",
    "if True in user_tagging_time[['userID', 'artistID']].duplicated():\n",
    "    print(\"Contains Duplicate user-artist combinations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e24324d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 3 random samples for tag data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>artistID</th>\n",
       "      <th>tagID</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>tagValue</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>166819</th>\n",
       "      <td>771</td>\n",
       "      <td>271</td>\n",
       "      <td>5394</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2010</td>\n",
       "      <td>lamangous hiphop</td>\n",
       "      <td>1270072800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22205</th>\n",
       "      <td>1433</td>\n",
       "      <td>3070</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2007</td>\n",
       "      <td>80s</td>\n",
       "      <td>1180648800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165165</th>\n",
       "      <td>692</td>\n",
       "      <td>2459</td>\n",
       "      <td>3779</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2009</td>\n",
       "      <td>novelty</td>\n",
       "      <td>1249077600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        userID  artistID  tagID  day  month  year          tagValue  \\\n",
       "166819     771       271   5394    1      4  2010  lamangous hiphop   \n",
       "22205     1433      3070     25    1      6  2007               80s   \n",
       "165165     692      2459   3779    1      8  2009           novelty   \n",
       "\n",
       "            timestamp  \n",
       "166819  1270072800000  \n",
       "22205   1180648800000  \n",
       "165165  1249077600000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_tag_a = user_tagging.merge(tags[['tagID', 'tagValue']], on='tagID')\n",
    "u_tag_a = u_tag_a.merge(user_tagging_time, on=['userID', 'artistID', 'tagID'])\n",
    "print(\"Displaying 3 random samples for tag data:\")\n",
    "u_tag_a.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42516c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group by user-artist combo, sort by timestamp and extract that tagValue\n",
    "u_tag_a = u_tag_a.sort_values(by='timestamp', ascending=False).groupby(['userID', 'artistID']).first().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d05ae16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does not contain Duplicate user-artist combinations.\n"
     ]
    }
   ],
   "source": [
    "#Let's check if this dataset contains duplicate user-artist combinations\n",
    "if True in np.unique(u_tag_a[['userID', 'artistID']].duplicated().values):\n",
    "    print(\"Contains Duplicate user-artist combinations.\")\n",
    "else:\n",
    "    print(\"Does not contain Duplicate user-artist combinations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1ba109",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "Our preprocessing steps are as before for the most part. For a final step, we will merge our tag information dataset with our ratings matrix. To start, we normalise our weight column as previous.\n",
    "\n",
    "There will be many cases where a user listens to a particular artist, but never provides that artist with a tag. In those cases, we will let the tag value be `no tag`, and for the corresponding timestamp value, we will use a value corresponding to today. We obviously want our model to find associations between users and common tags. However, our model can also build associations in situations where a user has decided to not provide a tag.\n",
    "\n",
    "The timestamps provided in the dataset do not correspond to the correct year and must have the final 3 digits removed. For this, we can just divide them all by 1,000. Using [this website](https://timestamp.online/), I entered some of the corrected timestamps to ensure they do indeed correspond to the appropriate year. All entries I checked returned values around 2008 to 2011, which makes sense for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3ace463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "#Correct timestamp data in u_tag_a\n",
    "u_tag_a['timestamp'] = u_tag_a['timestamp'].apply(lambda x: math.floor(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3830e37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's define our amount of users\n",
    "rating_matrix = pd.read_csv('../data/user_artists.dat', sep='\\t', encoding='latin-1')\n",
    "num_users = len(rating_matrix.userID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75b82646",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanc\\AppData\\Local\\Temp/ipykernel_15160/1239368141.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  user_ratings['weight'] = tf.keras.utils.normalize(ratings, axis=-1, order=2)[0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>92834.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.091235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.109804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.030397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.062543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.109109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             weight\n",
       "count  92834.000000\n",
       "mean       0.091235\n",
       "std        0.109804\n",
       "min        0.000008\n",
       "25%        0.030397\n",
       "50%        0.062543\n",
       "75%        0.109109\n",
       "max        1.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's normalise our weight column per user\n",
    "new_rating_matrix = pd.DataFrame(columns=['userID', 'artistID', 'weight'])\n",
    "for user_id in rating_matrix.userID.unique():\n",
    "    user_ratings = rating_matrix[rating_matrix.userID == user_id]\n",
    "    ratings = np.array(user_ratings['weight'])\n",
    "    user_ratings['weight'] = tf.keras.utils.normalize(ratings, axis=-1, order=2)[0]\n",
    "    new_rating_matrix = new_rating_matrix.append(user_ratings)\n",
    "rating_matrix = new_rating_matrix\n",
    "rating_matrix.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55f9ac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's use left merge to merge our tag data and rating matrix\n",
    "rating_matrix = rating_matrix.merge(u_tag_a[['userID', 'artistID', 'tagValue', 'timestamp']],\n",
    "                                    on=['userID', 'artistID'], how='left')\n",
    "\n",
    "#Get today's timestamp\n",
    "now = math.floor(time.time())\n",
    "\n",
    "#Fill missing values as stated\n",
    "rating_matrix.tagValue = rating_matrix['tagValue'].fillna('no tag')\n",
    "rating_matrix.timestamp = rating_matrix['timestamp'].fillna(now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcbcdd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying Sample of new Rating Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>artistID</th>\n",
       "      <th>weight</th>\n",
       "      <th>tagValue</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35651</th>\n",
       "      <td>779</td>\n",
       "      <td>9978</td>\n",
       "      <td>0.034552</td>\n",
       "      <td>german</td>\n",
       "      <td>1.267398e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26720</th>\n",
       "      <td>580</td>\n",
       "      <td>6602</td>\n",
       "      <td>0.085549</td>\n",
       "      <td>emo</td>\n",
       "      <td>1.214863e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83173</th>\n",
       "      <td>1879</td>\n",
       "      <td>1980</td>\n",
       "      <td>0.058302</td>\n",
       "      <td>alternative</td>\n",
       "      <td>1.251756e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70209</th>\n",
       "      <td>1582</td>\n",
       "      <td>10799</td>\n",
       "      <td>0.154664</td>\n",
       "      <td>totalmente foda</td>\n",
       "      <td>1.207001e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3484</th>\n",
       "      <td>74</td>\n",
       "      <td>2050</td>\n",
       "      <td>0.024876</td>\n",
       "      <td>caruso</td>\n",
       "      <td>1.238537e+12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      userID artistID    weight         tagValue     timestamp\n",
       "35651    779     9978  0.034552           german  1.267398e+12\n",
       "26720    580     6602  0.085549              emo  1.214863e+12\n",
       "83173   1879     1980  0.058302      alternative  1.251756e+12\n",
       "70209   1582    10799  0.154664  totalmente foda  1.207001e+12\n",
       "3484      74     2050  0.024876           caruso  1.238537e+12"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Displaying Sample of new Rating Matrix\")\n",
    "rating_matrix[rating_matrix.tagValue != 'no tag'].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22451696",
   "metadata": {},
   "source": [
    "The small sample above gives an indication for some of the values we can expect to find for tags. There is a large amount of distinct values in our tag data. It will be interesting to see how the recommender system interprets these.\n",
    "\n",
    "The below pre-processing steps are as before in our other notebooks. We are correcting the scale of user and artist ID's, then ensuring their maximum values are appropriate before replacing the columns in our rating matrix.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15926ad",
   "metadata": {},
   "source": [
    "### Artist Preprocessing\n",
    "To make use of the tags generally associated with artists, we will calculate their most popular tag in our data. We will use this information later on when developing our candidate model. The function in the cell below performs as previous. Essentially, it finds the most popular tag for each artist and attaches it to their profile.\n",
    "\n",
    "We will add this extra information to our ratings matrix, as well as the artists name. Using the artist name as an identifier will make more sense to us than an ID number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a886bd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's match artists to genres\n",
    "artists = pd.read_csv('../data/artists.dat', sep='\\t', encoding='latin-1')\n",
    "artists_tagged = user_tagging.merge(tags[['tagID', 'tagValue']], on='tagID')\n",
    "artists_tagged = (artists_tagged.groupby('artistID')['tagValue'].apply(lambda grp: list(grp))).reset_index()\n",
    "\n",
    "#This function performs as previous.\n",
    "for index, row in artists_tagged.iterrows():\n",
    "    d = {}\n",
    "    new_tags = []\n",
    "    for val in row.tagValue:\n",
    "        if val not in d:\n",
    "            d[val] = 1\n",
    "        else:\n",
    "            d[val] += 1\n",
    "    for key, value in d.items():\n",
    "        if d[key] >=3:\n",
    "            new_tags.append([key, value])\n",
    "    new_tags.sort(key=lambda x:x[1], reverse=True)\n",
    "    if new_tags:\n",
    "        artists_tagged.at[index, \"tagValue\"] = [tag[0] for tag in new_tags]\n",
    "        artists_tagged.at[index, 'genre'] = artists_tagged.at[index, 'tagValue'][0]\n",
    "        \n",
    "#Let's add these tags to our artists\n",
    "artists.rename(columns={'id':'artistID'}, inplace=True)\n",
    "artists = artists.join(artists_tagged, on='artistID', how='left', rsuffix='right')\n",
    "artists.tagValue = artists.tagValue.fillna('No Tags')\n",
    "artists.genre = artists.genre.fillna('No Tags')\n",
    "artists.rename(columns={'tagValue': 'genres'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "912db48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We add the extra info to our ratings matrix\n",
    "rating_matrix = rating_matrix.merge(artists[['artistID', 'name', 'genre']], on='artistID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71cc426e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract userID column\n",
    "userids = np.asarray(rating_matrix.userID)\n",
    "\n",
    "#Remap the column\n",
    "u_mapper, u_ind = np.unique(userids, return_inverse=True)\n",
    "\n",
    "#Let's define our amount of artists\n",
    "artists = pd.read_csv('../data/artists.dat', sep='\\t', encoding='latin-1')\n",
    "artists.rename(columns={'id':'artistID'}, inplace=True)\n",
    "num_artists = len(artists.artistID.unique())\n",
    "\n",
    "#Extract artistID column\n",
    "artistids = np.asarray(rating_matrix.artistID)\n",
    "\n",
    "#Remap the column\n",
    "a_mapper, a_ind = np.unique(artistids, return_inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b69e0996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's replace old columns with new ind ones\n",
    "rating_matrix.userID = u_ind\n",
    "rating_matrix.artistID = a_ind\n",
    "\n",
    "#Let's ensure the max value is approriate\n",
    "assert(rating_matrix.userID.unique().max() == 1891)\n",
    "assert(rating_matrix.artistID.unique().max() == 17631)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f62a0776",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We convert the ID's to string so we can use the StringLookup function later\n",
    "rating_matrix.userID = rating_matrix.userID.apply(str)\n",
    "rating_matrix.artistID = rating_matrix.artistID.apply(str)\n",
    "\n",
    "rating_matrix.timestamp = rating_matrix.timestamp.apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b4a6d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>artistID</th>\n",
       "      <th>weight</th>\n",
       "      <th>tagValue</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>name</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>73024</th>\n",
       "      <td>693</td>\n",
       "      <td>5452</td>\n",
       "      <td>0.054310</td>\n",
       "      <td>korea</td>\n",
       "      <td>1270072800000</td>\n",
       "      <td>SS501</td>\n",
       "      <td>No Tags</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40776</th>\n",
       "      <td>672</td>\n",
       "      <td>983</td>\n",
       "      <td>0.036924</td>\n",
       "      <td>no tag</td>\n",
       "      <td>1638562992</td>\n",
       "      <td>Chris Rea</td>\n",
       "      <td>electronic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4559</th>\n",
       "      <td>1590</td>\n",
       "      <td>151</td>\n",
       "      <td>0.058214</td>\n",
       "      <td>no tag</td>\n",
       "      <td>1638562992</td>\n",
       "      <td>Michael Jackson</td>\n",
       "      <td>new wave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28735</th>\n",
       "      <td>1651</td>\n",
       "      <td>534</td>\n",
       "      <td>0.034917</td>\n",
       "      <td>no tag</td>\n",
       "      <td>1638562992</td>\n",
       "      <td>Fall Out Boy</td>\n",
       "      <td>hip-hop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80249</th>\n",
       "      <td>871</td>\n",
       "      <td>8458</td>\n",
       "      <td>0.078178</td>\n",
       "      <td>no tag</td>\n",
       "      <td>1638562992</td>\n",
       "      <td>The Cooper Temple Clause</td>\n",
       "      <td>No Tags</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      userID artistID    weight tagValue      timestamp  \\\n",
       "73024    693     5452  0.054310    korea  1270072800000   \n",
       "40776    672      983  0.036924   no tag     1638562992   \n",
       "4559    1590      151  0.058214   no tag     1638562992   \n",
       "28735   1651      534  0.034917   no tag     1638562992   \n",
       "80249    871     8458  0.078178   no tag     1638562992   \n",
       "\n",
       "                           name       genre  \n",
       "73024                     SS501     No Tags  \n",
       "40776                 Chris Rea  electronic  \n",
       "4559            Michael Jackson    new wave  \n",
       "28735              Fall Out Boy     hip-hop  \n",
       "80249  The Cooper Temple Clause     No Tags  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_matrix.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f52b9e1",
   "metadata": {},
   "source": [
    "## Model Development\n",
    "We will perform the steps to developing a model as previous. However, we will now utilise our tags and timestamps. We do this by instantiating our interactions dictionary and including the extra features.\n",
    "\n",
    "### Instantiate Interaction Dictionary\n",
    "Our interactions dictionary is just our rating matrix. It contains the following features `userID`, `artistID`, `name`, `weight`, `tag`, `genre`, and `timestamp`. We create a mapping for our dictionary below. We also create seperate individual mappings for artist ID's, tags, and genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e693e1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's build our interactions dictionary as previous\n",
    "interactions_dict = {name: np.array(value) for name, value in rating_matrix.items()}\n",
    "interactions = tf.data.Dataset.from_tensor_slices(interactions_dict)\n",
    "\n",
    "items_dict = rating_matrix[['artistID']].drop_duplicates()\n",
    "items_dict = {name: np.array(value) for name, value in items_dict.items()}\n",
    "items = tf.data.Dataset.from_tensor_slices(items_dict)\n",
    "\n",
    "names_dict = rating_matrix[['name']].drop_duplicates()\n",
    "names_dict = {name: np.array(value) for name, value in names_dict.items()}\n",
    "names = tf.data.Dataset.from_tensor_slices(names_dict)\n",
    "\n",
    "tags_dict = rating_matrix[['tagValue']].drop_duplicates()\n",
    "tags_dict = {name: np.array(value) for name, value in tags_dict.items()}\n",
    "tags = tf.data.Dataset.from_tensor_slices(tags_dict)\n",
    "\n",
    "genre_dict = rating_matrix[['genre']].drop_duplicates()\n",
    "genre_dict = {name:np.array(value) for name, value in genre_dict.items()}\n",
    "genres = tf.data.Dataset.from_tensor_slices(genre_dict)\n",
    "\n",
    "interactions = interactions.map(lambda x: {\n",
    "                                            'userID' : x['userID'], \n",
    "                                            'artistID' : x['artistID'], \n",
    "                                            'name' : x['name'],\n",
    "                                            'weight' : float(x['weight']),\n",
    "                                            'tag' : x['tagValue'],\n",
    "                                            'genre': x['genre'],\n",
    "                                            'timestamp': x[\"timestamp\"],})\n",
    "\n",
    "#artists = names.map(lambda x: x['name'])\n",
    "items = items.map(lambda x: x['artistID'])\n",
    "tags = tags.map(lambda x: x['tagValue'])\n",
    "genres = genres.map(lambda x: x['genre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520cdadc",
   "metadata": {},
   "source": [
    "### Timestamp Normalisation\n",
    "As timestamps are represented as large integers, they are not healthy to use as direct input into our model. We firstly normalise our timestamps by calculaitng our minimum and maximum timestamp, then creating buckets at equal intervals between these two times. We instantiate 1000 buckets which are used to host our timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72b780cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's create bins for our timestamps\n",
    "max_timestamp = interactions.map(lambda x: x[\"timestamp\"]).reduce(\n",
    "    tf.cast(0, tf.int64), tf.maximum).numpy().max()\n",
    "\n",
    "min_timestamp = interactions.map(lambda x: x[\"timestamp\"]).reduce(\n",
    "    np.int64(1e9), tf.minimum).numpy().min()\n",
    "\n",
    "timestamp_buckets = np.linspace( min_timestamp, max_timestamp, num=1000,)\n",
    "\n",
    "timestamps = interactions.map(lambda x: x[\"timestamp\"]).batch(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e82e1c",
   "metadata": {},
   "source": [
    "### Lookup Tables & Training, Test Data Split\n",
    "In the following cell, we define various lookup tables which we may use later on. We also shuffle our data and create testing and training batches which will be fed into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4bb7993a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### get unique item and user id's as a lookup table\n",
    "unique_artist_ids = (np.unique(a_ind)).astype(str)\n",
    "unique_user_ids = (np.unique(u_ind)).astype(str)\n",
    "unique_genre_ids = np.unique(rating_matrix.genre)\n",
    "unique_user_tags = np.unique(rating_matrix.tagValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4a94b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly shuffle data and split between train and test.\n",
    "tf.random.set_seed(42)\n",
    "shuffled = interactions.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(62_000)\n",
    "test = shuffled.skip(62_000).take(30_000)\n",
    "\n",
    "cached_train = train.shuffle(62_000).batch(5_000)\n",
    "cached_test = test.batch(2_500).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b705f69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our test set is: 62000\n",
      "our train set is: 30000\n"
     ]
    }
   ],
   "source": [
    "print(f'our test set is: {len(train)}')\n",
    "print(f'our train set is: {len(test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0043bd7f",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "The below cells host a more complex version of the model we saw previously. In our previous model, we simply instantiated our user and item embeddings as we would in a regular collaborative filtering model. In this instance, we further develop our user and item models.\n",
    "\n",
    "---\n",
    "\n",
    "### User Model\n",
    "In the below cell, we develop our user model. We incorporate the user ID, as well as the timestamp data. As the timestamp data signifies when a user provided a tag to an artist, it is more suitably found in the user model.\n",
    "\n",
    "In our user model, we have included a parameter, `_use_timestamps`. When set to true, the model incorporates time stamp information. This will allow us to compare the results of the model with, or without the use of timestamps.\n",
    "\n",
    "In our dataset, it's hard to interpret the utility of timestamps. This is because timestamp information is related to when the user-provided tags were actually applied to the artist, rather than when the user posted the tag. Also, there is an argument that including timestamps of today's date may have negative effects on the model. This model is trained on information from 2009 to 2011, essentially making it a model of that period. Timestamps from today allow the model to 'see into the future', which is obviously not a realistic trait of ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3511dd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "### user model\n",
    "\n",
    "class UserModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, use_timestamps):\n",
    "        super().__init__()\n",
    "        max_tokes=25_000\n",
    "\n",
    "        self._use_timestamps = use_timestamps\n",
    "\n",
    "        ## embed user id from unique_user_ids\n",
    "        self.user_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "                vocabulary=unique_user_ids),\n",
    "            tf.keras.layers.Embedding(len(unique_user_ids) + 1, 32),\n",
    "        ])\n",
    "        \n",
    "        ## embed timestamp\n",
    "        if use_timestamps:\n",
    "            self.timestamp_embedding = tf.keras.Sequential([\n",
    "              tf.keras.layers.experimental.preprocessing.Discretization(timestamp_buckets.tolist()),\n",
    "              tf.keras.layers.Embedding(len(timestamp_buckets) + 1, 32),\n",
    "            ])\n",
    "            self.normalized_timestamp = tf.keras.layers.experimental.preprocessing.Normalization(axis=None)\n",
    "            self.normalized_timestamp.adapt(timestamps)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if not self._use_timestamps:\n",
    "              return self.user_embedding(inputs[\"userID\"])\n",
    "\n",
    "        ## all features here\n",
    "        return tf.concat([\n",
    "            self.user_embedding(inputs[\"userID\"]),\n",
    "            self.timestamp_embedding(inputs[\"timestamp\"]),\n",
    "            tf.reshape(self.normalized_timestamp(inputs[\"timestamp\"]), (-1, 1)),\n",
    "        ], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10663b35",
   "metadata": {},
   "source": [
    "### Item Model\n",
    "Our item model incorporates our artist ID as before. However, it also makes use of the genre associated with the artist. \n",
    "\n",
    "To make use of genre strings, we must first instantiate our `genre_vectorizer`. This will allow us to convert our genre string into a numerical representation. The `genre_vectorizer` is then used by our `genre_text_embedding` processing step to create an embedding of this word vector. Word embeddings allow us to measure similarity between text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1081375",
   "metadata": {},
   "outputs": [],
   "source": [
    "### candidate model\n",
    "\n",
    "class ItemModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        max_tokens = 10_000\n",
    "        \n",
    "        ## embed artist id from unique_artist_ids\n",
    "        self.artist_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "                vocabulary=unique_artist_ids),\n",
    "            tf.keras.layers.Embedding(len(unique_artist_ids) + 1, 32),])\n",
    "        \n",
    "        ## processing text features: item genre vectorizer\n",
    "        self.artist_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "            max_tokens=max_tokens)\n",
    "\n",
    "        ## we apply genre vectorizer to genres\n",
    "        self.artist_text_embedding = tf.keras.Sequential([\n",
    "                              self.artist_vectorizer,\n",
    "                              tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),\n",
    "                              tf.keras.layers.GlobalAveragePooling1D(),])\n",
    "\n",
    "        self.artist_vectorizer.adapt(genres)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.concat([\n",
    "            self.artist_embedding(inputs[\"artistID\"]),\n",
    "            self.artist_text_embedding(inputs[\"genre\"]),], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2e2a5e",
   "metadata": {},
   "source": [
    "### Combining Models\n",
    "The following cell is our parent model which we use to combine the output of both our User and Item models. We feed the outputs of each model into two dense embedding layers both of the same shape (*32*). \n",
    "\n",
    "We then define our task (in this case FactorizedTopK), then compute the loss as we did previously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9789ee96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicModel(tfrs.models.Model):\n",
    "    def __init__(self, use_timestamps):\n",
    "        super().__init__()\n",
    "\n",
    "        ## query model is user model\n",
    "        self.query_model = tf.keras.Sequential([\n",
    "                          UserModel(use_timestamps),\n",
    "                          tf.keras.layers.Dense(32)])\n",
    "        \n",
    "        ## candidate model is the item model\n",
    "        self.candidate_model = tf.keras.Sequential([\n",
    "                              ItemModel(),\n",
    "                              tf.keras.layers.Dense(32)])\n",
    "        \n",
    "        ## retrieval task, choose metrics\n",
    "        self.task = tfrs.tasks.Retrieval(\n",
    "                    metrics=tfrs.metrics.FactorizedTopK(\n",
    "                        candidates=items.batch(128).map(self.candidate_model),),)\n",
    "\n",
    "    def compute_loss(self, features, training=False):\n",
    "        # We only pass the user id and timestamp features into the query model. This\n",
    "        # is to ensure that the training inputs would have the same keys as the\n",
    "        # query inputs. Otherwise the discrepancy in input structure would cause an\n",
    "        # error when loading the query model after saving it.\n",
    "        \n",
    "        query_embeddings = self.query_model({ \"userID\": features[\"userID\"],\n",
    "                                               \"timestamp\": features[\"timestamp\"],\n",
    "                                                \"tag\": features[\"tag\"],\n",
    "                                            })\n",
    "        \n",
    "        item_embeddings = self.candidate_model(features[\"genre\"])\n",
    "\n",
    "        return self.task(query_embeddings, item_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071b5984",
   "metadata": {},
   "source": [
    "### Model Fitting and Evaluation\n",
    "In the following cells, we will perform two different fitting and evaluation scenarios: $(a)$ `_use_timestamps = True`, and $(b)$ `_use_timestamps = False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b060342b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Exception encountered when calling layer \"item_model\" (type ItemModel).\n\nin user code:\n\n    File \"C:\\Users\\seanc\\AppData\\Local\\Temp/ipykernel_15160/3669663950.py\", line 30, in call  *\n        self.artist_text_embedding(inputs[\"genre\"]),], axis=1)\n\n    TypeError: Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got 'artistID'\n\n\nCall arguments received:\n  â€¢ inputs=tf.Tensor(shape=(None,), dtype=string)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15160/2711739428.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMusicModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muse_timestamps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdagrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcached_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcached_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15160/2758195486.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, use_timestamps)\u001b[0m\n\u001b[0;32m     16\u001b[0m         self.task = tfrs.tasks.Retrieval(\n\u001b[0;32m     17\u001b[0m                     metrics=tfrs.metrics.FactorizedTopK(\n\u001b[1;32m---> 18\u001b[1;33m                         candidates=items.batch(128).map(self.candidate_model),),)\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[0;32m   2002\u001b[0m         warnings.warn(\"The `deterministic` argument has no effect unless the \"\n\u001b[0;32m   2003\u001b[0m                       \"`num_parallel_calls` argument is specified.\")\n\u001b[1;32m-> 2004\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2005\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2006\u001b[0m       return ParallelMapDataset(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[0;32m   5453\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_use_inter_op_parallelism\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5454\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_preserve_cardinality\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5455\u001b[1;33m     self._map_func = StructuredFunctionWrapper(\n\u001b[0m\u001b[0;32m   5456\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5457\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m   4531\u001b[0m         \u001b[0mfn_factory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrace_tf_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefun_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4533\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4534\u001b[0m     \u001b[1;31m# There is no graph to add in eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4535\u001b[0m     \u001b[0madd_to_graph\u001b[0m \u001b[1;33m&=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3242\u001b[0m          \u001b[1;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensorSpec\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3243\u001b[0m     \"\"\"\n\u001b[1;32m-> 3244\u001b[1;33m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0m\u001b[0;32m   3245\u001b[0m         *args, **kwargs)\n\u001b[0;32m   3246\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3208\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3209\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3210\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3211\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3555\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3556\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3557\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3558\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3559\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3390\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3391\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3392\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3393\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3394\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[0;32m   1141\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1143\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1145\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   4508\u001b[0m           attributes=defun_kwargs)\n\u001b[0;32m   4509\u001b[0m       \u001b[1;32mdef\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=missing-docstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4510\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrapper_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4511\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4512\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_helper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   4438\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_should_unpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4439\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4440\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4441\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0m_should_pack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4442\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    694\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 696\u001b[1;33m           \u001b[1;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    697\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_allowlisted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 383\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m   \u001b[1;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 464\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    465\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    697\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m           \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m           \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Exception encountered when calling layer \"item_model\" (type ItemModel).\n\nin user code:\n\n    File \"C:\\Users\\seanc\\AppData\\Local\\Temp/ipykernel_15160/3669663950.py\", line 30, in call  *\n        self.artist_text_embedding(inputs[\"genre\"]),], axis=1)\n\n    TypeError: Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got 'artistID'\n\n\nCall arguments received:\n  â€¢ inputs=tf.Tensor(shape=(None,), dtype=string)"
     ]
    }
   ],
   "source": [
    "model = MusicModel(use_timestamps=False)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "model.fit(cached_train, epochs=3)\n",
    "model.evaluate(cached_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1471c302",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'userID': <tf.Tensor 'IteratorGetNext:5' shape=(None,) dtype=string>, 'timestamp': <tf.Tensor 'IteratorGetNext:4' shape=(None,) dtype=int64>, 'tag': <tf.Tensor 'IteratorGetNext:3' shape=(None,) dtype=string>}\n",
      "Consider rewriting this model with the Functional API.\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'userID': <tf.Tensor 'IteratorGetNext:5' shape=(None,) dtype=string>, 'timestamp': <tf.Tensor 'IteratorGetNext:4' shape=(None,) dtype=int64>, 'tag': <tf.Tensor 'IteratorGetNext:3' shape=(None,) dtype=string>}\n",
      "Consider rewriting this model with the Functional API.\n",
      "13/13 [==============================] - 62s 4s/step - factorized_top_k/top_1_categorical_accuracy: 0.2838 - factorized_top_k/top_5_categorical_accuracy: 0.3035 - factorized_top_k/top_10_categorical_accuracy: 0.3124 - factorized_top_k/top_50_categorical_accuracy: 0.3349 - factorized_top_k/top_100_categorical_accuracy: 0.3480 - loss: 39029.6232 - regularization_loss: 0.0000e+00 - total_loss: 39029.6232\n",
      "Epoch 2/3\n",
      "13/13 [==============================] - 60s 4s/step - factorized_top_k/top_1_categorical_accuracy: 0.3954 - factorized_top_k/top_5_categorical_accuracy: 0.4175 - factorized_top_k/top_10_categorical_accuracy: 0.4273 - factorized_top_k/top_50_categorical_accuracy: 0.4529 - factorized_top_k/top_100_categorical_accuracy: 0.4650 - loss: 38513.8764 - regularization_loss: 0.0000e+00 - total_loss: 38513.8764\n",
      "Epoch 3/3\n",
      "13/13 [==============================] - 63s 5s/step - factorized_top_k/top_1_categorical_accuracy: 0.5219 - factorized_top_k/top_5_categorical_accuracy: 0.5371 - factorized_top_k/top_10_categorical_accuracy: 0.5441 - factorized_top_k/top_50_categorical_accuracy: 0.5613 - factorized_top_k/top_100_categorical_accuracy: 0.5698 - loss: 38298.1392 - regularization_loss: 0.0000e+00 - total_loss: 38298.1392\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'userID': <tf.Tensor 'IteratorGetNext:5' shape=(None,) dtype=string>, 'timestamp': <tf.Tensor 'IteratorGetNext:4' shape=(None,) dtype=int64>, 'tag': <tf.Tensor 'IteratorGetNext:3' shape=(None,) dtype=string>}\n",
      "Consider rewriting this model with the Functional API.\n",
      "12/12 [==============================] - 30s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.4899 - factorized_top_k/top_5_categorical_accuracy: 0.5035 - factorized_top_k/top_10_categorical_accuracy: 0.5103 - factorized_top_k/top_50_categorical_accuracy: 0.5257 - factorized_top_k/top_100_categorical_accuracy: 0.5324 - loss: 19461.5293 - regularization_loss: 0.0000e+00 - total_loss: 19461.5293\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'factorized_top_k/top_1_categorical_accuracy': 0.4898666739463806,\n",
       " 'factorized_top_k/top_5_categorical_accuracy': 0.5034999847412109,\n",
       " 'factorized_top_k/top_10_categorical_accuracy': 0.5102666616439819,\n",
       " 'factorized_top_k/top_50_categorical_accuracy': 0.5256999731063843,\n",
       " 'factorized_top_k/top_100_categorical_accuracy': 0.5324333310127258,\n",
       " 'loss': 19464.4296875,\n",
       " 'regularization_loss': 0,\n",
       " 'total_loss': 19464.4296875}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MusicModel(use_timestamps=True)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "model.fit(cached_train, epochs=3)\n",
    "model.evaluate(cached_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25440a7",
   "metadata": {},
   "source": [
    "### Output Analysis\n",
    "From the above output, it's clear that timestamps do not improve recommendations for this model. This is likely due to the fact that a substantial amount of the timestamps are of today's date which is throwing off the model. Perhaps a better data imputation would have been the median date observed in the data.\n",
    "\n",
    "Otherwise both models seem to have reasonable performance with a positive item being returned as the top candidate 50% of the time. These models will supply a basis for our next advanced deep retrieval model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}