{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a73f3610",
   "metadata": {},
   "source": [
    "# Collaborative Filtering - Matrix Factorization\n",
    "In this Notebook, we will be performing collaborative filtering using matrix factorization. The steps we will follow are detailed in the introduction. We will be using TensorFlow as our ML framework for the development of our music recommender system. Let's begin by building a sparse representation of our ratings matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdc6cc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ece26a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some convenience functions to Pandas DataFrame.\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "def mask(df, key, function):\n",
    "    \"\"\"Returns a filtered dataframe, by applying function to key\"\"\"\n",
    "    return df[function(df[key])]\n",
    "\n",
    "def flatten_cols(df):\n",
    "    df.columns = [' '.join(col).strip() for col in df.columns.values]\n",
    "    return df\n",
    "\n",
    "pd.DataFrame.mask = mask\n",
    "pd.DataFrame.flatten_cols = flatten_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e80a62b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's define our amount of users and artists\n",
    "rating_matrix = pd.read_csv('../data/user_artists.dat', sep='\\t', encoding='latin-1')\n",
    "num_users = len(rating_matrix.userID.unique())\n",
    "\n",
    "artists = pd.read_csv('../data/artists.dat', sep='\\t', encoding='latin-1')\n",
    "num_artists = len(artists.id.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0724bad",
   "metadata": {},
   "source": [
    "## Sparse Representation of $A$\n",
    "Let's define a helper function which will build a sparse representation of our ratings matrix $A$. We will use TensorFlow `SparseTensor` to generate our sparse representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa408716",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to build sparse representation of rating matrix\n",
    "def build_rating_sparse_tensor(ratings_df):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    ratings_df: a pd.DataFrame with `userID`, `artistID` and `weight` columns.\n",
    "    Returns:\n",
    "    a tf.SparseTensor representing the ratings matrix.\n",
    "    \"\"\"\n",
    "    indices = ratings_df[['userID', 'artistID']].values\n",
    "    values = ratings_df['weight'].values\n",
    "    return tf.SparseTensor(\n",
    "        indices=indices,\n",
    "        values=values,\n",
    "        dense_shape=[num_users, num_artists])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe987f0",
   "metadata": {},
   "source": [
    "## Error Function\n",
    "Let's define our error function. We will be using **Mean Square Error** (MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d644e190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_mean_square_error(sparse_ratings, user_embeddings, movie_embeddings):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sparse_ratings: A SparseTensor rating matrix, of dense_shape [N, M]\n",
    "        user_embeddings: A dense Tensor U of shape [N, k] where k is the embedding\n",
    "        dimension, such that U_i is the embedding of user i.\n",
    "        movie_embeddings: A dense Tensor V of shape [M, k] where k is the embedding\n",
    "        dimension, such that V_j is the embedding of movie j.\n",
    "    Returns:\n",
    "        A scalar Tensor representing the MSE between the true ratings and the\n",
    "        model's predictions.\n",
    "    \"\"\"\n",
    "    predictions = tf.gather_nd(\n",
    "        tf.matmul(user_embeddings, movie_embeddings, transpose_b=True),\n",
    "        sparse_ratings.indices)\n",
    "    loss = tf.losses.mean_squared_error(sparse_ratings.values, predictions)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f872f9a2",
   "metadata": {},
   "source": [
    "## CFModel Helper Class\n",
    "Below you will find a useful helper class for collaborative filtering extracted from the [Google Recommendation System Colab notebook](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/recommendation-systems/recommendation-systems.ipynb?utm_source=ss-recommendation-systems&utm_campaign=colab-external&utm_medium=referral&utm_content=recommendation-systems#scrollTo=TOVDyYHgo4th). It is a simple class for training a CF Model using *Stochastic Gradient Descent* (SGD). As is explained in the introduction, we will not be using *Alternating Least Squares* (ALS) as the benefits of the algorithm will not be recognised in our approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d45bd60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFModel(object):\n",
    "    \"\"\"Simple class that represents a collaborative filtering model\"\"\"\n",
    "    def __init__(self, embedding_vars, loss, metrics=None):\n",
    "        \"\"\"Initializes a CFModel.\n",
    "        Args:\n",
    "          embedding_vars: A dictionary of tf.Variables.\n",
    "          loss: A float Tensor. The loss to optimize.\n",
    "          metrics: optional list of dictionaries of Tensors. The metrics in each\n",
    "            dictionary will be plotted in a separate figure during training.\n",
    "        \"\"\"\n",
    "        self._embedding_vars = embedding_vars\n",
    "        self._loss = loss\n",
    "        self._metrics = metrics\n",
    "        self._embeddings = {k: None for k in embedding_vars}\n",
    "        self._session = None\n",
    "\n",
    "    @property\n",
    "    def embeddings(self):\n",
    "        \"\"\"The embeddings dictionary.\"\"\"\n",
    "        return self._embeddings\n",
    "\n",
    "    def train(self, num_iterations=100, learning_rate=1.0, plot_results=True,\n",
    "            optimizer=tf.train.GradientDescentOptimizer):\n",
    "        \"\"\"Trains the model.\n",
    "        Args:\n",
    "          iterations: number of iterations to run.\n",
    "          learning_rate: optimizer learning rate.\n",
    "          plot_results: whether to plot the results at the end of training.\n",
    "          optimizer: the optimizer to use. Default to GradientDescentOptimizer.\n",
    "        Returns:\n",
    "          The metrics dictionary evaluated at the last iteration.\n",
    "        \"\"\"\n",
    "        with self._loss.graph.as_default():\n",
    "            opt = optimizer(learning_rate)\n",
    "            train_op = opt.minimize(self._loss)\n",
    "            local_init_op = tf.group(\n",
    "                tf.variables_initializer(opt.variables()),\n",
    "                tf.local_variables_initializer())\n",
    "            if self._session is None:\n",
    "                self._session = tf.Session()\n",
    "                with self._session.as_default():\n",
    "                    self._session.run(tf.global_variables_initializer())\n",
    "                    self._session.run(tf.tables_initializer())\n",
    "                    tf.train.start_queue_runners()\n",
    "\n",
    "        with self._session.as_default():\n",
    "            local_init_op.run()\n",
    "            iterations = []\n",
    "            metrics = self._metrics or ({},)\n",
    "            metrics_vals = [collections.defaultdict(list) for _ in self._metrics]\n",
    "\n",
    "            # Train and append results.\n",
    "            for i in range(num_iterations + 1):\n",
    "                _, results = self._session.run((train_op, metrics))\n",
    "                if (i % 10 == 0) or i == num_iterations:\n",
    "                    print(\"\\r iteration %d: \" % i + \", \".join(\n",
    "                        [\"%s=%f\" % (k, v) for r in results for k, v in r.items()]),\n",
    "                        end='')\n",
    "                    iterations.append(i)\n",
    "                    for metric_val, result in zip(metrics_vals, results):\n",
    "                        for k, v in result.items():\n",
    "                            metric_val[k].append(v)\n",
    "\n",
    "            for k, v in self._embedding_vars.items():\n",
    "                self._embeddings[k] = v.eval()\n",
    "\n",
    "            if plot_results:\n",
    "                # Plot the metrics.\n",
    "                num_subplots = len(metrics)+1\n",
    "                fig = plt.figure()\n",
    "                fig.set_size_inches(num_subplots*10, 8)\n",
    "                for i, metric_vals in enumerate(metrics_vals):\n",
    "                    ax = fig.add_subplot(1, num_subplots, i+1)\n",
    "                    for k, v in metric_vals.items():\n",
    "                        ax.plot(iterations, v, label=k)\n",
    "                    ax.set_xlim([1, num_iterations])\n",
    "                    ax.legend()\n",
    "            return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbc762ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(ratings, embedding_dim=3, init_stddev=1.):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        ratings: a DataFrame of the ratings\n",
    "        embedding_dim: the dimension of the embedding vectors.\n",
    "        init_stddev: float, the standard deviation of the random initial embeddings.\n",
    "    Returns:\n",
    "        model: a CFModel.\n",
    "    \"\"\"\n",
    "    # Split the ratings DataFrame into train and test.\n",
    "    train_ratings, test_ratings = split_dataframe(ratings)\n",
    "    \n",
    "    # SparseTensor representation of the train and test datasets.\n",
    "    A_train = build_rating_sparse_tensor(train_ratings)\n",
    "    A_test = build_rating_sparse_tensor(test_ratings)\n",
    "    \n",
    "    # Initialize the embeddings using a normal distribution.\n",
    "    U = tf.Variable(tf.random_normal(\n",
    "        [A_train.dense_shape[0], embedding_dim], stddev=init_stddev))\n",
    "    V = tf.Variable(tf.random_normal(\n",
    "        [A_train.dense_shape[1], embedding_dim], stddev=init_stddev))\n",
    "    train_loss = sparse_mean_square_error(A_train, U, V)\n",
    "    test_loss = sparse_mean_square_error(A_test, U, V)\n",
    "    metrics = {\n",
    "        'train_error': train_loss,\n",
    "        'test_error': test_loss\n",
    "    }\n",
    "    embeddings = {\n",
    "        \"user_id\": U,\n",
    "        \"movie_id\": V\n",
    "    }\n",
    "    return CFModel(embeddings, train_loss, [metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256a94dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}