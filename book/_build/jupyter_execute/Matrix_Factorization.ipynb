{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8537589d",
   "metadata": {},
   "source": [
    "# Collaborative Filtering - Matrix Factorization\n",
    "In this Notebook, we will be performing collaborative filtering using matrix factorization. The steps we will follow are detailed in the introduction. We will be using TensorFlow as our ML framework for the development of our music recommender system. Let's begin by building a sparse representation of our ratings matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f085a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "import collections\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from IPython import display\n",
    "import sklearn\n",
    "import sklearn.manifold\n",
    "from matplotlib import pyplot as plt\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a508ddf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some convenience functions to Pandas DataFrame.\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "def mask(df, key, function):\n",
    "    \"\"\"Returns a filtered dataframe, by applying function to key\"\"\"\n",
    "    return df[function(df[key])]\n",
    "\n",
    "def flatten_cols(df):\n",
    "    df.columns = [' '.join(col).strip() for col in df.columns.values]\n",
    "    return df\n",
    "\n",
    "pd.DataFrame.mask = mask\n",
    "pd.DataFrame.flatten_cols = flatten_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0455e18",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f900a470",
   "metadata": {},
   "source": [
    "In the following cells, we will be re-mapping our user ID's to fit into a scale that is defined as being greater than 0, and less than or equal to the number of unique users. Currently, despite there being only 1,892 unique users, profiles exist with ID's such as 1,893, 2,000, and so on. Mapping our user ID's to a suitable scale is good practice and will avoid issues in the future.\n",
    "\n",
    "The exact same principle will be applied to the artist ID's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78e1d8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's define our amount of users\n",
    "rating_matrix = pd.read_csv('../data/user_artists.dat', sep='\\t', encoding='latin-1')\n",
    "num_users = len(rating_matrix.userID.unique())\n",
    "\n",
    "#Extract userID column\n",
    "userids = np.asarray(rating_matrix.userID)\n",
    "\n",
    "#Remap the column\n",
    "u_mapper, u_ind = np.unique(userids, return_inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b3e2a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's define our amount of artists\n",
    "artists = pd.read_csv('../data/artists.dat', sep='\\t', encoding='latin-1')\n",
    "num_artists = len(artists.id.unique())\n",
    "\n",
    "#Extract artistID column\n",
    "artistids = np.asarray(rating_matrix.artistID)\n",
    "\n",
    "#Remap the column\n",
    "a_mapper, a_ind = np.unique(artistids, return_inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "493d538e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assert that u_ind and userID column are of same size\n",
    "assert(len(u_ind) == len(rating_matrix.userID))\n",
    "\n",
    "#Assert that a_ind and artistID column are of same size\n",
    "assert(len(a_ind) == len(rating_matrix.artistID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2506c9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's replace old columns with new ind ones\n",
    "rating_matrix.userID = u_ind\n",
    "rating_matrix.artistID = a_ind\n",
    "\n",
    "#Let's ensure the max value is approriate\n",
    "assert(rating_matrix.userID.unique().max() == 1891)\n",
    "assert(rating_matrix.artistID.unique().max() == 17631)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82966b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>artistID</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>92834.000</td>\n",
       "      <td>92834.000</td>\n",
       "      <td>92834.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>944.222</td>\n",
       "      <td>3235.737</td>\n",
       "      <td>745.244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>546.751</td>\n",
       "      <td>4197.217</td>\n",
       "      <td>3751.322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>470.000</td>\n",
       "      <td>430.000</td>\n",
       "      <td>107.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>944.000</td>\n",
       "      <td>1237.000</td>\n",
       "      <td>260.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1416.000</td>\n",
       "      <td>4266.000</td>\n",
       "      <td>614.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1891.000</td>\n",
       "      <td>17631.000</td>\n",
       "      <td>352698.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         userID  artistID     weight\n",
       "count 92834.000 92834.000  92834.000\n",
       "mean    944.222  3235.737    745.244\n",
       "std     546.751  4197.217   3751.322\n",
       "min       0.000     0.000      1.000\n",
       "25%     470.000   430.000    107.000\n",
       "50%     944.000  1237.000    260.000\n",
       "75%    1416.000  4266.000    614.000\n",
       "max    1891.000 17631.000 352698.000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_matrix.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289f7b9c",
   "metadata": {},
   "source": [
    "Our 'ratings' value consists of the number of listens a user has for a particular artist. These values can range from 1 all the way up to 352,698. Having inputs that vary this greatly reduces the profficiency of our model. Therefore, we will normalise the values using Keras normalize, with `order=2`. This means the inputs are normalized so that the summation of the normalized inputs squared is equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "854561ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max value recorded: 352698\n"
     ]
    }
   ],
   "source": [
    "#What is the max amount of listens\n",
    "print(f'The max value recorded: {rating_matrix.weight.max()}')\n",
    "\n",
    "\n",
    "#normalize the weight array\n",
    "rating_matrix.weight = tf.keras.utils.normalize(np.asarray(rating_matrix.weight), order=2)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e14420",
   "metadata": {},
   "source": [
    "## Sparse Representation of $A$\n",
    "Let's define a helper function which will build a sparse representation of our ratings matrix $A$. We will use TensorFlow `SparseTensor` to generate our sparse representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c3954d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to build sparse representation of rating matrix\n",
    "def build_rating_sparse_tensor(ratings_df):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    ratings_df: a pd.DataFrame with `userID`, `artistID` and `weight` columns.\n",
    "    Returns:\n",
    "    a tf.SparseTensor representing the ratings matrix.\n",
    "    \"\"\"\n",
    "    indices = ratings_df[['userID', 'artistID']].values\n",
    "    values = ratings_df['weight'].values\n",
    "    return tf.SparseTensor(\n",
    "        indices=indices,\n",
    "        values=values,\n",
    "        dense_shape=[num_users, num_artists])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10303545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility to split the data into training and test sets.\n",
    "def split_dataframe(df, holdout_fraction=0.1):\n",
    "    \"\"\"Splits a DataFrame into training and test sets.\n",
    "    Args:\n",
    "        df: a dataframe.\n",
    "        holdout_fraction: fraction of dataframe rows to use in the test set.\n",
    "    Returns:\n",
    "        train: dataframe for training\n",
    "        test: dataframe for testing\n",
    "    \"\"\"\n",
    "    test = df.sample(frac=holdout_fraction, replace=False)\n",
    "    train = df[~df.index.isin(test.index)]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b0041d",
   "metadata": {},
   "source": [
    "## Error Function\n",
    "Let's define our error function. We will be using **Mean Square Error** (MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4bba1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_mean_square_error(sparse_ratings, query_embeddings, item_embeddings):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sparse_ratings: A SparseTensor rating matrix, of dense_shape [N, M]\n",
    "        query_embeddings: A dense Tensor U of shape [N, k] where k is the embedding\n",
    "        dimension, such that U_i is the embedding of user i.\n",
    "        item_embeddings: A dense Tensor V of shape [M, k] where k is the embedding\n",
    "        dimension, such that V_j is the embedding of movie j.\n",
    "    Returns:\n",
    "        A scalar Tensor representing the MSE between the true ratings and the\n",
    "        model's predictions.\n",
    "    \"\"\"\n",
    "    predictions = tf.gather_nd(\n",
    "        tf.matmul(query_embeddings, item_embeddings, transpose_b=True),\n",
    "        sparse_ratings.indices)\n",
    "    loss = tf.losses.mean_squared_error(sparse_ratings.values, predictions)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3c6f3a",
   "metadata": {},
   "source": [
    "## CFModel Helper Class\n",
    "Below you will find a useful helper class for collaborative filtering extracted from the [Google Recommendation System Colab notebook](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/recommendation-systems/recommendation-systems.ipynb?utm_source=ss-recommendation-systems&utm_campaign=colab-external&utm_medium=referral&utm_content=recommendation-systems#scrollTo=TOVDyYHgo4th). It is a simple class for training a CF Model using *Stochastic Gradient Descent* (SGD). As is explained in the introduction, we will not be using *Alternating Least Squares* (ALS) as the benefits of the algorithm will not be recognised in our approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edfc0ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFModel(object):\n",
    "    \"\"\"Simple class that represents a collaborative filtering model\"\"\"\n",
    "    def __init__(self, embedding_vars, loss, metrics=None):\n",
    "        \"\"\"Initializes a CFModel.\n",
    "        Args:\n",
    "          embedding_vars: A dictionary of tf.Variables.\n",
    "          loss: A float Tensor. The loss to optimize.\n",
    "          metrics: optional list of dictionaries of Tensors. The metrics in each\n",
    "            dictionary will be plotted in a separate figure during training.\n",
    "        \"\"\"\n",
    "        self._embedding_vars = embedding_vars\n",
    "        self._loss = loss\n",
    "        self._metrics = metrics\n",
    "        self._embeddings = {k: None for k in embedding_vars}\n",
    "        self._session = None\n",
    "\n",
    "    @property\n",
    "    def embeddings(self):\n",
    "        \"\"\"The embeddings dictionary.\"\"\"\n",
    "        return self._embeddings\n",
    "\n",
    "    def train(self, num_iterations=100, learning_rate=1.0, plot_results=True,\n",
    "            optimizer=tf.train.GradientDescentOptimizer):\n",
    "        \"\"\"Trains the model.\n",
    "        Args:\n",
    "          iterations: number of iterations to run.\n",
    "          learning_rate: optimizer learning rate.\n",
    "          plot_results: whether to plot the results at the end of training.\n",
    "          optimizer: the optimizer to use. Default to GradientDescentOptimizer.\n",
    "        Returns:\n",
    "          The metrics dictionary evaluated at the last iteration.\n",
    "        \"\"\"\n",
    "        with self._loss.graph.as_default():\n",
    "            opt = optimizer(learning_rate)\n",
    "            train_op = opt.minimize(self._loss)\n",
    "            local_init_op = tf.group(\n",
    "                tf.variables_initializer(opt.variables()),\n",
    "                tf.local_variables_initializer())\n",
    "            if self._session is None:\n",
    "                self._session = tf.Session()\n",
    "                with self._session.as_default():\n",
    "                    self._session.run(tf.global_variables_initializer())\n",
    "                    self._session.run(tf.tables_initializer())\n",
    "                    tf.train.start_queue_runners()\n",
    "\n",
    "        with self._session.as_default():\n",
    "            local_init_op.run()\n",
    "            iterations = []\n",
    "            metrics = self._metrics or ({},)\n",
    "            metrics_vals = [collections.defaultdict(list) for _ in self._metrics]\n",
    "\n",
    "            # Train and append results.\n",
    "            for i in range(num_iterations + 1):\n",
    "                _, results = self._session.run((train_op, metrics))\n",
    "                if (i % 10 == 0) or i == num_iterations:\n",
    "                    print(\"\\r iteration %d: \" % i + \", \".join(\n",
    "                        [\"%s=%f\" % (k, v) for r in results for k, v in r.items()]),\n",
    "                        end='')\n",
    "                    iterations.append(i)\n",
    "                    for metric_val, result in zip(metrics_vals, results):\n",
    "                        for k, v in result.items():\n",
    "                            metric_val[k].append(v)\n",
    "\n",
    "            for k, v in self._embedding_vars.items():\n",
    "                self._embeddings[k] = v.eval()\n",
    "\n",
    "            if plot_results:\n",
    "                # Plot the metrics.\n",
    "                num_subplots = len(metrics)+1\n",
    "                fig = plt.figure()\n",
    "                fig.set_size_inches(num_subplots*10, 8)\n",
    "                for i, metric_vals in enumerate(metrics_vals):\n",
    "                    ax = fig.add_subplot(1, num_subplots, i+1)\n",
    "                    for k, v in metric_vals.items():\n",
    "                        ax.plot(iterations, v, label=k)\n",
    "                    ax.set_xlim([1, num_iterations])\n",
    "                    ax.legend()\n",
    "            return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97c831d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(ratings, embedding_dim=3, init_stddev=1.):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        ratings: a DataFrame of the ratings\n",
    "        embedding_dim: the dimension of the embedding vectors.\n",
    "        init_stddev: float, the standard deviation of the random initial embeddings.\n",
    "    Returns:\n",
    "        model: a CFModel.\n",
    "    \"\"\"\n",
    "    # Split the ratings DataFrame into train and test.\n",
    "    train_ratings, test_ratings = split_dataframe(ratings)\n",
    "    \n",
    "    # SparseTensor representation of the train and test datasets.\n",
    "    A_train = build_rating_sparse_tensor(train_ratings)\n",
    "    A_test = build_rating_sparse_tensor(test_ratings)\n",
    "    \n",
    "    # Initialize the embeddings using a normal distribution.\n",
    "    U = tf.Variable(tf.random_normal(\n",
    "        [A_train.dense_shape[0], embedding_dim], stddev=init_stddev))\n",
    "    V = tf.Variable(tf.random_normal(\n",
    "        [A_train.dense_shape[1], embedding_dim], stddev=init_stddev))\n",
    "    train_loss = sparse_mean_square_error(A_train, U, V)\n",
    "    test_loss = sparse_mean_square_error(A_test, U, V)\n",
    "    metrics = {\n",
    "        'train_error': train_loss,\n",
    "        'test_error': test_loss\n",
    "    }\n",
    "    embeddings = {\n",
    "        \"user_id\": U,\n",
    "        \"movie_id\": V\n",
    "    }\n",
    "    return CFModel(embeddings, train_loss, [metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19c27c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\seanc\\AppData\\Local\\Temp/ipykernel_18056/560305088.py:44: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " iteration 0: train_error=1.883014, test_error=1.863988"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " iteration 10: train_error=1.533526, test_error=1.635659"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " iteration 20: train_error=1.286834, test_error=1.473491"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " iteration 30: train_error=1.102290, test_error=1.351431"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " iteration 40: train_error=0.958659, test_error=1.255673"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " iteration 50: train_error=0.843613, test_error=1.178186"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " iteration 60: train_error=0.749433, test_error=1.113966"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18056/1635380472.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrating_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_stddev\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18056/560305088.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, num_iterations, learning_rate, plot_results, optimizer)\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[1;31m# Train and append results.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_iterations\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m                 \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m                     print(\"\\r iteration %d: \" % i + \", \".join(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    955\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 957\u001b[1;33m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[0;32m    958\u001b[0m                          run_metadata_ptr)\n\u001b[0;32m    959\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;31m# or if the call is a partial run that specifies feeds.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1180\u001b[1;33m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0m\u001b[0;32m   1181\u001b[0m                              feed_dict_tensor, options, run_metadata)\n\u001b[0;32m   1182\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1358\u001b[1;33m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0m\u001b[0;32m   1359\u001b[0m                            run_metadata)\n\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1363\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1347\u001b[0m       \u001b[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1349\u001b[1;33m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0m\u001b[0;32m   1350\u001b[0m                                       target_list, run_metadata)\n\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1439\u001b[0m   def _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list,\n\u001b[0;32m   1440\u001b[0m                           run_metadata):\n\u001b[1;32m-> 1441\u001b[1;33m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0m\u001b[0;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1443\u001b[0m                                             run_metadata)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = build_model(rating_matrix, embedding_dim=30, init_stddev=0.5)\n",
    "model.train(num_iterations=1000, learning_rate=10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b765673f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}